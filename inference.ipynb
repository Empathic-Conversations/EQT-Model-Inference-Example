{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint models/intent/ckpt-3 restored!!!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict, Callable, Any\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TF info/warning messages\n",
    "\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from pytorch_transformers import RobertaTokenizer\n",
    "\n",
    "from src.datasets import create_test_dataset_for_prediction\n",
    "from src.model_qbert import QBERT\n",
    "from src.model_utils import create_masks\n",
    "\n",
    "# Select intent prediction or type prediction\n",
    "class_type = 'intent'\n",
    "checkpoints_path = f\"models/{class_type}\"\n",
    "restore_epoch = 3 if class_type == 'intent' else 4\n",
    "\n",
    "# From argmax indices to human-readable labels\n",
    "mappings = {\n",
    "    'type': {\n",
    "        'Ask about antecedent':  0,\n",
    "        'Ask about consequence': 1,\n",
    "        'Ask for confirmation':  2,\n",
    "        'Irony':                 3,\n",
    "        'Negative rhetoric':     4,\n",
    "        'Positive rhetoric':     5,\n",
    "        'Request information':   6,\n",
    "        'Suggest a reason':      7,\n",
    "        'Suggest a solution':    8,\n",
    "    },\n",
    "    'intent': {\n",
    "        'Amplify excitement': 0,\n",
    "        'Amplify joy':        1,\n",
    "        'Amplify pride':      2,\n",
    "        'De-escalate':        3,\n",
    "        'Express concern':    4,\n",
    "        'Express interest':   5,\n",
    "        'Moralize speaker':   6,\n",
    "        'Motivate':           7,\n",
    "        'Offer relief':       8,\n",
    "        'Pass judgement':     9,\n",
    "        'Support':           10,\n",
    "        'Sympathize':        11,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load model checkpoint; the parameters don't matter for inference\n",
    "#   - Mostly copied from original code repository\n",
    "num_layers         = 12\n",
    "d_model            = 768\n",
    "num_heads          = 12\n",
    "dff                = d_model * 4\n",
    "hidden_act         = \"gelu\"\n",
    "dropout_rate       = 0.1\n",
    "layer_norm_eps     = 1e-5\n",
    "max_position_embed = 514\n",
    "\n",
    "tokenizer  = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "lab_mapping  = mappings[class_type]\n",
    "pred_mapping = {v: k for k, v in lab_mapping.items()}\n",
    "num_classes  = len(pred_mapping.keys())\n",
    "\n",
    "adam_beta_1  = 0.9\n",
    "adam_beta_2  = 0.98\n",
    "adam_epsilon = 1e-6\n",
    "\n",
    "qbert = QBERT(num_layers, \n",
    "              d_model, \n",
    "              num_heads, \n",
    "              dff, \n",
    "              hidden_act, \n",
    "              dropout_rate,\n",
    "              layer_norm_eps, \n",
    "              max_position_embed, \n",
    "              vocab_size, \n",
    "              num_classes)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(2e-5, \n",
    "                                            beta_1=adam_beta_1, \n",
    "                                            beta_2=adam_beta_2,\n",
    "                                            epsilon=adam_epsilon)\n",
    "ckpt = tf.train.Checkpoint(model=qbert, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoints_path, max_to_keep=None)\n",
    "ckpt.restore(ckpt_manager.checkpoints[restore_epoch - 1]).expect_partial()\n",
    "print('Checkpoint {} restored!!!'.format(ckpt_manager.checkpoints[restore_epoch - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_dataset: tf.data.Dataset) -> Tuple[List[str], List[int]]:\n",
    "    \"\"\"Main function for inference. Directly copied from \n",
    "        original code repository.\"\"\"\n",
    "\n",
    "    import tqdm\n",
    "    \n",
    "    y_pred = []\n",
    "    pred_ids = []\n",
    "    for inputs in tqdm.tqdm(test_dataset):\n",
    "        inp, weights, ids = inputs\n",
    "        enc_padding_mask = create_masks(inp)\n",
    "        pred_class = qbert(inp, weights, False, enc_padding_mask)  # Inference\n",
    "        pred_class = np.argmax(pred_class.numpy(), axis=1)\n",
    "        y_pred += pred_class.tolist()\n",
    "        y_pred_lab = [pred_mapping[pred] for pred in y_pred]\n",
    "        pred_labels = np.array(y_pred_lab)\n",
    "        pred_ids += ids.numpy().squeeze().tolist()\n",
    "\n",
    "    return pd.DataFrame({'id': pred_ids, f'predicted_{class_type}_label': pred_labels})\n",
    "\n",
    "\n",
    "def format_dialogue(\n",
    "    dialogue: List[Dict[str, str]], \n",
    "    sep_char: str = \"\\n\",\n",
    "    key_content: str = \"text\"\n",
    ") -> str:\n",
    "    \"\"\"Simply concatenate the utterances of dialogue turns with a \n",
    "        `sep` character, which defaults to the newline character \n",
    "        to conform the EQT provided API `create_test_dataset_for_prediction()`.\n",
    "        \n",
    "        In the data-preparation function `create_test_dataset_for_prediction()`,\n",
    "        the original authors will split the dialogue by `\\n` and REVERSE THE ORDER.\n",
    "        So, the utterances of LAST SPEAKER WILL BE THE FIRST. They claim that this \n",
    "        makes the model pay more attention to the sentence that we actually want to predict.\"\"\"\n",
    "    \n",
    "    ret = sep_char.join(d[key_content] for d in dialogue)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def generate_samples_from_dialogue(\n",
    "    dialogue: List[Dict[str, str]],\n",
    "    q_predictor: Callable[[str], bool] = None,\n",
    "    key_content: str = \"text\",\n",
    "    key_role: str = \"role\",\n",
    "    role_bot: str = \"bot\",\n",
    ") -> List[str]:\n",
    "    \"\"\"For each input dialogue turns in the form of `List[Dict[str, str]]`,\n",
    "        generate samples that comply with the EQT inference API.\n",
    "        \n",
    "        If multiple questions present in a single dialogue turn, e.g., the bot\n",
    "        asks two questions in a role, then we generate two samples.\"\"\"\n",
    "\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    \n",
    "    # Can supply a custom question predictor\n",
    "    #   - If not supplied, assume that the bot's utterance \n",
    "    #     is a question if it ends with a question mark\n",
    "    q_predictor = q_predictor or (lambda x: x.endswith(\"?\"))\n",
    "\n",
    "    ret = []\n",
    "    for i, d in enumerate(dialogue, start=1):\n",
    "        if d[key_role] == role_bot:\n",
    "            dialogue_truncated = dialogue[:i]\n",
    "\n",
    "            # Generate one sample for each question in the bot's utterance\n",
    "            #   - If a question is not detected, generate one sample by \n",
    "            #     truncating the bot's utterance at the question sentence\n",
    "            sentences_bot = sent_tokenize(d[key_content])\n",
    "            for j, s in enumerate(sentences_bot, start=1):\n",
    "                if q_predictor(s):\n",
    "                    d_cp = d.copy()\n",
    "                    d_cp[key_content] = \" \".join(sentences_bot[:j])\n",
    "                    sample = dialogue_truncated[:-1] + [d_cp]\n",
    "                    ret.append(format_dialogue(sample, key_content=key_content))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"human\",\n",
      "        \"text\": \"Hal, I'm here because I need help in understanding my daughter's relationship with her girlfriend, and how to support her. I really care about my daughter and I want her to be happy, but I'm also worried about how my relatives and friends will perceive her and the relationship. I know it's important to support her and to break away from family norms, but I'm struggling with how to do that. Can you help me?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"bot\",\n",
      "        \"text\": \"I'm sorry to hear that this is such a difficult situation. It sounds like you have a lot of conflicting feelings and pressures. Can you tell me what your biggest concerns are in this situation, and how you think I can help?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"human\",\n",
      "        \"text\": \"My biggest concern is how to reconcile my traditional beliefs with supporting my daughter's relationship. I want her to be happy, but I'm also afraid of how her relationship might be seen within my community. I'm hoping you can help me find a way to be supportive of her and her relationship while still respecting our family's cultural values.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"bot\",\n",
      "        \"text\": \"I understand your conflict. It's a difficult situation for so many families. I'd like to ask some questions to better understand how you may be able to move forward in a way that respects and honors both your daughter's relationship and your family's culture. Is there anything specific that stands out for you when it comes to this challenge?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"human\",\n",
      "        \"text\": \"Yes, I'm not sure how to express my feelings of acceptance to my daughter without sacrificing my traditional beliefs. I'm not sure if I should even bring up the topic, let alone how to talk about it with her. I don't want to make her feel like her relationship isn't valid, but I'm also afraid of what will happen if I accept it openly.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"bot\",\n",
      "        \"text\": \"It sounds like you are trying to find a way to be supportive while still remaining true to your traditional beliefs. I'd suggest first taking some time for yourself to reflect on the situation and your feelings towards it. \\nThis can allow you to gain clarity so that when you do talk with your daughter, you can come from a place of understanding and compassion. It might also be helpful to focus on the values that define who you are as opposed to those imposed by family or societal norms.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"human\",\n",
      "        \"text\": \"That makes sense. I think I'd like to take some time to process everything before talking with my daughter. I appreciate your advice and I think it will be helpful.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"bot\",\n",
      "        \"text\": \"I'm glad that my advice is helpful to you. Is there anything else I can do to support you in this process?\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Load the example dialogues. Each dialogue turn is represented as \n",
    "#   a dictionary with keys \"text\" and \"role\".\n",
    "with open('data/two_dialogues.json', 'r') as f:\n",
    "    source_dialogues: List[List[Dict[str, str]]] = json.load(f)\n",
    "    print(json.dumps(source_dialogues[0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is 50265.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 711.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 9 examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>utterance_truncated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hal, I'm here because I need help in understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hal, I'm here because I need help in understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hal, I'm here because I need help in understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                utterance_truncated\n",
       "0   0  Hal, I'm here because I need help in understan...\n",
       "1   1  Hal, I'm here because I need help in understan...\n",
       "2   2  Hal, I'm here because I need help in understan...\n",
       "3   3  Hi Hal, I'm in a really tough spot right now. ...\n",
       "4   4  Hi Hal, I'm in a really tough spot right now. ...\n",
       "5   5  Hi Hal, I'm in a really tough spot right now. ...\n",
       "6   6  Hi Hal, I'm in a really tough spot right now. ...\n",
       "7   7  Hi Hal, I'm in a really tough spot right now. ...\n",
       "8   8  Hi Hal, I'm in a really tough spot right now. ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_samples = generate_samples_from_dialogue(source_dialogues[0]) + generate_samples_from_dialogue(source_dialogues[1])\n",
    "df_samples = pd.DataFrame({'utterance_truncated': ls_samples}).reset_index().rename(columns={'index': 'id'})  # Needs to use these column names to work with the EQT API\n",
    "ds_samples = create_test_dataset_for_prediction(tokenizer, df_samples, 32, 256, lab_mapping)\n",
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted_intent_label</th>\n",
       "      <th>utterance_truncated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Offer relief</td>\n",
       "      <td>Hal, I'm here because I need help in understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Express interest</td>\n",
       "      <td>Hal, I'm here because I need help in understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Offer relief</td>\n",
       "      <td>Hal, I'm here because I need help in understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Offer relief</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Offer relief</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Offer relief</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Offer relief</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Offer relief</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Express interest</td>\n",
       "      <td>Hi Hal, I'm in a really tough spot right now. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id predicted_intent_label  \\\n",
       "0   0           Offer relief   \n",
       "1   1       Express interest   \n",
       "2   2           Offer relief   \n",
       "3   3           Offer relief   \n",
       "4   4           Offer relief   \n",
       "5   5           Offer relief   \n",
       "6   6           Offer relief   \n",
       "7   7           Offer relief   \n",
       "8   8       Express interest   \n",
       "\n",
       "                                 utterance_truncated  \n",
       "0  Hal, I'm here because I need help in understan...  \n",
       "1  Hal, I'm here because I need help in understan...  \n",
       "2  Hal, I'm here because I need help in understan...  \n",
       "3  Hi Hal, I'm in a really tough spot right now. ...  \n",
       "4  Hi Hal, I'm in a really tough spot right now. ...  \n",
       "5  Hi Hal, I'm in a really tough spot right now. ...  \n",
       "6  Hi Hal, I'm in a really tough spot right now. ...  \n",
       "7  Hi Hal, I'm in a really tough spot right now. ...  \n",
       "8  Hi Hal, I'm in a really tough spot right now. ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = predict(ds_samples)\n",
    "df_predictions = pd.merge(df_predictions, df_samples, on='id', how='left')\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eqt-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
